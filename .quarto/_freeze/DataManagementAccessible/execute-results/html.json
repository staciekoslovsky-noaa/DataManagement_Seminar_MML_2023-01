{
  "hash": "423bd4768ddcd6eb93b42a67f8c084ad",
  "result": {
    "markdown": "---\ntitle: \"Data Management Made a Little Less Scary\"\nsubtitle: \"Strategies to Meet You Where You Are\"\nauthor: \"Stacie Koslovsky (Hardy), MML/PEP\"\ndate: '2023-01-13'\nformat: revealjs\nlogo: https://www.fisheries.noaa.gov/themes/custom/noaa_components/images/fisheries_header_logo_jul2019.png\nfooter: \"MML Seminar Series | Data Management Made a Little Less Scary | Stacie Koslovsky (Hardy)\"\n---\n\n\n## Goals for Seminar {.smaller}\n\n-   Define data management and data manager/scientist role.\n\n-   Share concepts and examples.\n\n-   Empower you to take this information back with you into your day-to-day work:\n\n    -   Start Small\n\n    -   Happy Medium\n\n    -   Bring It On!\n\n![](DataManagementAccessible_files/images/data_are_plural.jpg){fig-align=\"center\"}\n\n## 2020 NOAA Data Strategy {.smaller}\n\n::: columns\n::: {.column width=\"25%\"}\n![](DataManagementAccessible_files/images/noaa_data_strategy.png)\n:::\n\n::: {.column width=\"75%\"}\n\"Data are at the heart of NOAA's \\$5 billion per year enterprise. ... NOAA data are a critical strategic asset used to ensure accountability, manage operations, and to maintain and enhance the performance of the economy, public health, and welfare.\"\n\n-   **Goal 1**: Align data management leadership roles across the organization.\n\n-   **Goal 2**: Govern and manage data strategically to most effectively steward the US taxpayers' investment.\n\n-   **Goal 3**: Share data as openly and widely as possible to promote maximum utilization of NOAA data.\n\n-   **Goal 4**: Promote data innovation and quality improvements to facilitate science and support data-driven decision making.\n\n-   **Goal 5**: Engage stakeholders and leverage partnerships to maximize the value of NOAA data to the Nation.\n:::\n:::\n\n## Overview {.smaller}\n\n![](http://www.phdcomics.com/comics/archive/phd053104s.gif){fig-align=\"center\"}\n\nThe data we collect, manage, and analyze today will continue to be available and used by staff in the future (and outside of NOAA). It is, therefore, important to maintain, organize and share our data in a way that will be useful and meaningful to current-us and future-us.\n\nBut, data storage is costly, in terms of space for storing data, space for backing up data, and time for managing, organizing and documenting data, and so on...\n\n*We want to be mindful in our data management strategies and application*.\n\n**At least 50% of data management is people-related.**\n\n## Ideally... {.smaller}\n\nA well-designed data management system will:\n\n-   Make archiving, storing, and retrieving information less difficult and tedious;\n\n-   Ensure the integrity and continuity of record keeping, *despite changes in personnel*;\n\n-   Allow for the easy identification and purging of outdated information;\n\n-   Allow us to more easily share our information...with managers, collaborators, the public;\n\n-   Be foundational;\n\n    -   The technologies we use are ever-changing. Think about the different waves of emerging technologies: GIS, genetics, stable isotopes, machine learning.\n\n    -   We can't predict what will come next, but if we manage our data well, it will be easier to adapt.\n\n-   Be uniform in implementation; and\n\n-   Be expandable and flexible to meet future needs.\n\n## Data Management Life Cycle\n\n\n::: {.cell warnings='false' height='5'}\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-9f32b4500c107f6a2f6d\" style=\"width:960px;height:480px;\" class=\"grViz html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-9f32b4500c107f6a2f6d\">{\"x\":{\"diagram\":\"digraph flowchart {\\n      # node definitions with substituted label text\\n      node [fontname = Arial, shape = rectangle, penwidth = 3]        \\n      tab1 [label = \\\"Plan (as best and as much as you can)\\\", color = MediumVioletRed]\\n      tab2 [label = \\\"Collect (the fun part, most of the time)\\\", color = Tomato]\\n      tab3 [label = \\\"Assure (quality assurance and control)\\\", color = Gold]\\n      tab4 [label = \\\"Describe (data documentation)\\\", color = YellowGreen]\\n      tab5 [label = \\\"Preserve (at data center or archive)\\\", color = DarkOliveGreen]\\n      tab6 [label = \\\"Discover (make your data findable)\\\", color = CadetBlue]\\n      tab7 [label = \\\"Integrate and Analyze (you know this part better than me)\\\", color = Navy]\\n\\n      # edge definitions with the node IDs\\n      tab1 -> tab2;\\n      tab2 -> tab3;\\n      tab3 -> tab4;\\n      tab4 -> tab5;\\n      tab5 -> tab6;\\n      tab6 -> tab7;\\n      tab7 -> tab1;\\n      }\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n\nAdopted from DataOne.\n:::\n:::\n\n\n## Data Management Life Cycle\n\n\n::: {.cell warnings='false' height='5'}\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-97bc7e998fd690d57a7f\" style=\"width:960px;height:480px;\" class=\"grViz html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-97bc7e998fd690d57a7f\">{\"x\":{\"diagram\":\"digraph flowchart {\\n      # node definitions with substituted label text\\n      node [fontname = Arial, shape = rectangle, penwidth = 3]        \\n      tab1 [label = \\\"Plan (as best and as much as you can)\\\", color = MediumVioletRed]\\n      tab2 [label = \\\"Collect (the fun part, most of the time)\\\", color = Tomato]\\n      tab3 [label = \\\"Assure (quality assurance and control)\\\", color = Gold]\\n      tab4 [label = \\\"Describe (data documentation)\\\", color = YellowGreen]\\n      tab5 [label = \\\"Preserve (at data center or archive)\\\", color = DarkOliveGreen]\\n      tab6 [label = \\\"Discover (make your data findable)\\\", color = CadetBlue]\\n      tab7 [label = \\\"Integrate and Analyze (you likely know this part better than me)\\\", color = Navy]\\n\\n      # edge definitions with the node IDs\\n      tab1 -> tab2;\\n      tab2 -> tab3;\\n      tab2 -> tab7 [color = Orange, penwidth = 4];\\n      tab3 -> tab4;\\n      tab3 -> tab1;\\n      tab3 -> tab7;\\n      tab4 -> tab5;\\n      tab5 -> tab6;\\n      tab6 -> tab7;\\n      tab7 -> tab1;\\n      tab7 -> tab4;\\n      tab7 -> tab2  [color = Red, penwidth = 4];\\n      tab2 -> tab7  [color = Red, penwidth = 4];\\n      }\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n\nUpdated to reflect common practices.\n:::\n:::\n\n\n## Plan {.smaller}\n\n-   Start your project and research off with good planning and design.\n\n-   Data management is just *one* part of this early planning (and the only focus of this presentation).\n\n-   Early planning takes a lot of work, but working through all these steps from the beginning will save you *so much* time later.\n\n![](DataManagementAccessible_files/images/planning.jpg){fig-align=\"center\"}\n\n## Plan: Things to Consider {.smaller}\n\n-   What data will be generated? What data need to be gathered from other places (within your program, within MML, from outside sources)?\n\n-   Where will data for the project be stored (on the network, in a database, on Google Drive, etc.)?\n\n    -   How will the data be organized? What does the folder structure look like? How will files be named?\n\n    -   What does the database structure look like? How will data be entered or imported?\n\n-   How will information about the data be preserved? (GitHub, meeting notes, important decisions, etc.)\n\n-   How will the data move through all the subsequent steps (collection, assurance, description, preservation, discovery, integration and analysis)?\n\n-   Who is responsible for doing what? (an important one!)\n\n## Plan: Folder Organization Ideas {.smaller}\n\n::: columns\n::: {.column .smaller width=\"70%\"}\n-   Consider how to organize folders (by date, project, species).\n\n    -   Create folders with a top-down hierarchy.\n\n    -   Organize images separately.\n\n    -   Don't store final data only on your laptop!\n\n-   If you're going to do a big reorganization, consider deleting and archiving things first.\n\n-   For program-wide efforts, work together as a team to come up with different ways to store data, test out some mock-ups, and settle on a solution that works best for the group.\n\n-   Establish a system for version control (e.g. file-naming, Git). Not everything needs version control. Delete obsolete versions of files.\n\n-   Maintain whatever system you come up with -- check in, adjust it, and clean it up annually.\n:::\n\n::: {.column width=\"30%\"}\n![](https://imgs.xkcd.com/comics/old_files.png)\n:::\n:::\n\n## Plan: Folder Organization: Personal {.smaller}\n\n::: columns\n**Personal File Management** (could be on computer or in user's folder)\n\n![](DataManagementAccessible_files/images/sample_folder_structure_personal.png){fig-align=\"center\"}\n:::\n\n## Plan: Folder Organization: Program {.smaller}\n\n::: columns\n::: {.column width=\"33%\"}\n**Main Folder Structure**\n\n![](DataManagementAccessible_files/images/sample_folder_network_1.png)\n:::\n\n::: {.column width=\"33%\"}\n**Subfolders within...Data**\n\n![](DataManagementAccessible_files/images/sample_folder_network_2.png)\n:::\n\n::: {.column width=\"33%\"}\n**...ProgramMgmt & Projects**\n\n![](DataManagementAccessible_files/images/sample_folder_network_3.png)\n:::\n:::\n\n## Plan: Google Drive Organization...oof {.smaller}\n\nThere are two camps here (and I've highlighted some considerations for each):\n\n-   Let your files be (exist wherever they exist) and share as-needed.\n\n    -   Requires little to no folder organization overhead.\n\n    -   Relies on the search functionality within Google Drive to find things.\n\n-   Organize your files like you would a folder structure on the network or on your computer.\n\n    -   Simpler for on-boarding new staff (who may not know what to search for).\n\n    -   Can provide a more clear framework for groups working together.\n\n    -   More overhead required for managing files.\n\nFor larger groups (at the program- or project-scale), I'd recommend organizing a centralized folder system and sharing within that, but either way, it's a good conversation to have.\n\n## Plan: Google Drive Organization {.smaller}\n\n**PEP Centralized Google Drive Folder Structure**\n\n![](DataManagementAccessible_files/images/pepGoogleDrive.png){fig-align=\"center\"}\n\n**LAN vs Google Drive**\n\n-   I can't tell you what is best for you or your project or your program.\n\n-   This should be a group discussion and decision.\n\n-   Document this for yourself and future users (read_me file, project manual, etc.)\n\n## Plan: File Naming {.smaller}\n\n::: columns\n::: {.column width=\"33%\"}\n![](https://imgs.xkcd.com/comics/documents.png)\n:::\n\n::: {.column .smaller width=\"67%\"}\nFile names ideally describe the project, file contents, location, and date -- and should be unique enough to stand alone as file descriptions.\n\n**File names should be**:\n\n-   Human-readable\n\n-   Machine-readable\n\n-   Plays well with default ordering (!)\n\n    -   This may mean different solutions for different projects/reasons.\n    -   Order by date, number, name, etc.\n    -   Most general to most specific.\n\n-   Same rules apply to folder names!\n:::\n:::\n\n## Plan: File Naming Conventions {.smaller}\n\n-   File names should be unique, simple, SHORT, and readable.\n\n-   Avoid using spaces. Alternative ways to break up file naming without spaces: CamelCase, lowerCamelCase, snake_case, CamelCase_PlusSnakeCase.\n\n-   Use only alpha-numeric characters. Avoid special characters.\n\n-   Use leading zeros with the numbers 0-9 for better sorting (e.g., 01, 02, 11, 12).\n\n-   Dates should formatted to support logical default ordering.\n\n-   Avoid unclear names like last, final2, final revised, etc.\n\n::: columns\n::: {.column width=\"50%\"}\n**Don't's**\n\n-   Plan 1.docx\n\n-   Project_DataMgmtPlan\\_*longDetails*.docx\n\n-   01JAN2023 or 1-1-2023\n:::\n\n::: {.column width=\"50%\"}\n**Do's**\n\n-   Project_DataMgmtPlan_v01.docx\n\n-   Project_DataMgmtPlan_ShorterDetails.docx\n\n-   20230101 or 2023-01-01 or 2023_01_01\n:::\n:::\n\n## Plan: Date/Time Formats {.smaller}\n\n::: columns\n::: {.column width=\"40%\"}\n![](https://imgs.xkcd.com/comics/iso_8601.png){width=\"350\"}\n:::\n\n::: {.column width=\"60%\"}\n-   In data and in folder/file names, dates should formatted to meet ISO 8601 standards:\n\n    -   YYYY-MM-DD\n\n    -   YYYYMMDD\n\n    -   YYYY-MM\n\n    -   YYYY-Www (e.g. 2023-W03) if you use week data\n\n    -   Thh:mm:ss.sss (using the 24-hour clock system)\n\n-   We work across time zones, which makes data collection and management particularly complicated. Consider standardizing date/time data to GMT.\n:::\n:::\n\n## Plan: Reproducibility {.smaller}\n\n::: columns\n::: {.column width=\"60%\"}\nThink about how you're going to...\n\n-   Process and document your data processing steps.\n\n    -   Code is a really nice way to do this. It also saves you time later, if you have to redo anything.\n\n    -   If that's scary or not in your toolkit, writing down the (detailed) steps in a (shared) document is also totally okay.\n\n-   Archive versions of the data used for analyses, if they cannot be otherwise recreated.\n\n-   Come up with a plan for what you need to do with your data to meet PARR requirements. Do this early, and it becomes an easy box-checking exercise later.\n:::\n\n::: {.column width=\"40%\"}\n![](https://imgs.xkcd.com/comics/is_it_worth_the_time.png)\n:::\n:::\n\n## Plan: Take-away Actions {.smaller}\n\n::: columns\n::: {.column width=\"70%\"}\n**Start Small**\n\n-   Pick one thing that you think would be most beneficial for you to try. Build incrementally.\n\n-   Don't try to do everything all at once or go back through all your older work. Start small with something new(ish).\n\n**Happy Medium**\n\n-   Work with other members of your project/program to start to bring more of these measures into your work.\n\n**Bring It On!**\n\n-   Develop a comprehensive data management plan for your project/program.\n\n-   Use common planning and structures across program projects.\n\n-   Clean up and reorganize your own/project/program file organization.\n:::\n\n::: {.column width=\"30%\"}\n![](DataManagementAccessible_files/images/to_do_list.jpg)\n:::\n:::\n\n## Collect {.smaller}\n\nHow you collect data heavily influences down-stream data processing. Set yourself up for success from the beginning!\n\n-   Use consistent methods for collecting data.\n\n    -   Set up a template for data storage.\n\n    -   Use datasheets or cloud-based data recording systems that mirror how data will be stored later.\n\n    -   Use the same format year-to-year, changing as little as possible.\n\n-   Use consistent data organization:\n\n    -   Wide format (spreadsheet format): each row represents a complete entry.\n\n    -   Long format (database format): one column defines the parameter and one column stores the value of the parameter; makes data easier to migrate to a database later!\n\n    -   Color-coding in spreadsheets is *great* for visualization...but it's not a substitute for organizing your data properly.\n\n## Collect {.smaller}\n\nHow you collect data heavily influences down-stream data processing. Set yourself up for success from the beginning!\n\n-   Atomize data (make sure there is only one piece of information in each row/column entry).\n\n    -   Don't store multiple bits of information in the same field.\n\n    -   Comments fields are okay, but if you're going to need to mine them later for more information, save yourself the extra work from the beginning...\n\n-   Keep your raw data raw. Preserve it, imperfections and all.\n\n    -   Use a separate process to clean the data, ideally scripted to make the process reproducible (more to come on this in the Assure section).\n\n    -   Work off the cleaned version of the data.\n\n-   Import raw or manually enter data into a database!\n\n## Collect: Data Collection Template\n\n::: columns\n::: {.column width=\"50%\"}\n<div>\n\n**Main Folder Structure**\n\n![](DataManagementAccessible_files/images/coastal_data_1.png)\n\n</div>\n:::\n\n::: {.column width=\"50%\"}\n<div>\n\n**Subfolder Structure**\n\n![](DataManagementAccessible_files/images/coastal_data_2.png)\n\n</div>\n:::\n:::\n\n## Collect: Wide vs Long Formats\n\n::: columns\n::: {.column width=\"50%\"}\n**Wide Format**\n\n![](DataManagementAccessible_files/images/sample_results_wide.png)\n:::\n\n::: {.column width=\"50%\"}\n**Long Format** (preferred)\n\n![](DataManagementAccessible_files/images/sample_results_long.png)\n:::\n:::\n\n## Collect: Measurements + Units {.smaller}\n\n-   If you're storing the data in a wide format (each measurement type gets it's own field)...\n\n    -   AND all the measurements are in the same unit, store the unit at the end of the field name (e.g. length_m).\n\n    -   AND the measurements are collected in different units, you could easily get yourself in trouble. Either (preferably) convert to all the same units and name the field accordingly (with units at the end) OR create another field in which you store the associated units.\n\n-   If you're storing the data in a long format (all measurements in the same field, with another field indicating the measurement type), either create a measurement unit field OR (in a database) related the measurements to a measurement type lookup table and store the unit information there.\n\n## Collect: Archive/Storage {.smaller}\n\n::: columns\n::: {.column width=\"70%\"}\n-   Store data in non-proprietary formats (when possible) that are easy to work with in programming software:\n\n    -   CSV\n\n    -   Text files\n\n    -   Database - benefit of related data and lookup values\n\n        -   PostgreSQL / SQL Server / Azure\n\n        -   Geodatabases\n\n-   Avoid long field names.\n\n-   Avoid using special characters and spaces in field names.\n\n-   Store spatial data in appropriate projections. Decimal degrees in WGS-84 are a good default option.\n:::\n\n::: {.column width=\"30%\"}\n**PEP Database Schemas**\n\n![](DataManagementAccessible_files/images/database_schemas.png)\n:::\n:::\n\n## Collect: Example #1 (glacial database) {.smaller}\n\n**PostgreSQL Database Structure** (data are atomized within fields and link across tables)\n\n![](DataManagementAccessible_files/images/glacial_backEnd.png){fig-align=\"center\"}\n\n## Collect: Example #1 (glacial database) {.smaller}\n\n**Data Entry Forms** (in Access)\n\n![](DataManagementAccessible_files/images/glacial_dataEntry.png){fig-align=\"center\"}\n\n## Collect: Example #1 (glacial database) {.smaller}\n\n**Process Tracking** (in Access)\n\n![](DataManagementAccessible_files/images/glacial_processTracking.png){fig-align=\"center\"}\n\n## Collect: Example #1 (glacial database) {.smaller}\n\n**Dataset Tracking** (in Access)\n\n![](DataManagementAccessible_files/images/glacial_datasetTracking.png){fig-align=\"center\"}\n\n## Collect: Take-away Actions {.smaller}\n\n::: columns\n::: {.column width=\"70%\"}\n**Start Small**\n\n-   Rethink how you collect and store data while in the field.\n\n-   Come up with a more standardized folder structure and file naming approach.\n\n**Happy Medium**\n\n-   Migrate your data to a long data format.\n\n-   Re-evaluate datasheets and other data collection strategies.\n\n**Bring It On!**\n\n-   Migrate data to a database.\n\n-   Develop cloud-based data collection tools for the field.\n\n-   Automate data collection, where possible.\n:::\n\n::: {.column width=\"30%\"}\n![](DataManagementAccessible_files/images/to_do_list.jpg)\n:::\n:::\n\n## Assure {.smaller}\n\n::: columns\n::: {.column width=\"70%\"}\nThis is an easy step to overlook. Doing it well takes time...\n\n**Things to look at in your data**:\n\n-   Consistency in values throughout data collection.\n\n-   Reasonable min-max, average, range values for each field (query-based or graphically).\n\n-   Missing data.\n\n-   Large gaps in data (spatial or temporal bias?).\n\n-   Double-checking data that were manually entered.\n\n-   Logical checks.\n\n-   Assign quality flags to records to \"remove\" bad records.\n\n**Skipping this step can be catastrophic...**\n:::\n\n::: {.column width=\"30%\"}\n![](https://imgs.xkcd.com/comics/assigning_numbers.png)\n:::\n:::\n\n## Assure: Example #1 (in-field report) {.smaller}\n\nReview field data to ensure there were no issues with initial data collection...\n\n::: columns\n::: {.column width=\"50%\"}\n![](DataManagementAccessible_files/images/bodyConditionReport_1.png){width=\"449\"}\n:::\n\n::: {.column width=\"50%\"}\n![](DataManagementAccessible_files/images/bodyConditionReport_2.png){width=\"531\"}\n:::\n:::\n\n## Assure: Example #2 (in database) {.smaller}\n\nIn a database, tables are where your data are stored. Queries are layers that exist on top of that table that summarize, filter. or order your data...they do *not* make another copy of the data. When your underlying data change, the data in the query also change automatically.\n\nUsing queries to standardize extraction and to complete quality checks...\n\n![](DataManagementAccessible_files/images/database_queries.png){fig-align=\"center\"}\n\n## Assure: Take-away Actions {.smaller}\n\n::: columns\n::: {.column width=\"70%\"}\n**Start Small**\n\n-   Review data/datasheets in the field to catch problems early.\n\n-   Think of some *new* ways to quality check your data (e.g., figures in Excel, manual review).\n\n**Happy Medium**\n\n-   Assure quality data *before* you collect or enter data (e.g., drop-down menus in a spreadsheet/database).\n\n-   Identify ways to differentiate missing and NULL values.\n\n**Bring It On!**\n\n-   Develop a systematic and automated quality assurance process to run on data after data collection (e.g. database queries, reports generated in R).\n:::\n\n::: {.column width=\"30%\"}\n![](DataManagementAccessible_files/images/to_do_list.jpg)\n:::\n:::\n\n## Describe {.smaller}\n\n::: columns\n::: {.column width=\"70%\"}\nThis is another step that takes a lot of time to do well, but your future self will be really appreciative of your current self doing a thorough job.\n\n-   Describe the data organization.\n\n-   Describe who did what and the appropriate contact information.\n\n-   Describe the scientific context.\n\n-   Describe the data and parameters.\n\nThis is doesn't just have to be done in InPort.\n\n\\*\\*Any *dataset* metadata records created in InPort are required to have their data available online **within one year** of when the metadata record was created. This does not apply to project-level metadata records (FYI).\\*\\*\n:::\n\n::: {.column width=\"30%\"}\n![](https://imgs.xkcd.com/comics/every_data_table.png)\n:::\n:::\n\n## Describe: Example #1 (InPort) {.smaller}\n\n**PEP Metadata Repository**\n\n![](DataManagementAccessible_files/images/inPort.png){fig-align=\"center\"}\n\n## Describe: Example #2 (dashboard) {.smaller}\n\n**Project management tracking within PEP dashboard**\n\n![](DataManagementAccessible_files/images/pep_dashboard.png){fig-align=\"center\"}\n\n## Describe: Example #3 (GitHub) {.smaller}\n\n**Project management on GitHub** (developed by Josh London)\n\n![](DataManagementAccessible_files/images/gitHub_harborSeals.png){fig-align=\"center\"}\n\n## Describe: Take-away Actions {.smaller}\n\n::: columns\n::: {.column width=\"70%\"}\n<div>\n\n**Start Small**\n\n-   Start a shared document for tracking processing steps.\n-   Start an on-going meeting notes document.\n\n**Happy Medium**\n\n-   Develop a Google Space for sharing information (rather than through email).\n\n-   Use Google Tasks to track work.\n\n**Bring It On!**\n\n-   Project and issue tracking on GitHub.\n-   Develop project/program-wide method for tracking information.\n\n</div>\n:::\n\n::: {.column width=\"30%\"}\n![](DataManagementAccessible_files/images/to_do_list.jpg)\n:::\n:::\n\n## Preserve/Discover {.smaller}\n\n::: columns\n-   Want to share data and make it find-able!\n-   Find the archive location that makes the *most sense* for your data.\n-   At a minimum, sharing your data online meets PARR requirements...but this can also be a really cool way to highlight or showcase a particular research project.\n\n::: {.column width=\"50%\"}\n**Data preserving considerations**:\n\n-   Identify what best to share.\n\n-   Use standard terminology (where applicable).\n\n-   Remove any PII or confidential information.\n\n-   Have data citation.\n\n-   Get DOI?\n:::\n\n::: {.column width=\"50%\"}\n**Data archives to consider**:\n\n-   NOAA Big Data Program\n\n-   Animal Telemetry Network (ATN)\n\n-   OBIS SEAMAP\n\n-   NCEI\n\n-   ArcGIS online\n\n-   GitHub (size limitations)\n\n-   With manuscript\n:::\n:::\n\n## Discover: Example #1 (AGOL) {.smaller}\n\n**Data Portal** (for sharing data with AK Regional Office)\n\n![](DataManagementAccessible_files/images/agol.png){fig-align=\"center\"}\n\n## Discover: Example #2 (Shiny App) {.smaller}\n\n**Interactive application** (for exploring harbor seal abundance estimates)\n\n![](DataManagementAccessible_files/images/coastalPv_ShinyApp.png)\n\n## Preserve/Discover: Take-away Actions {.smaller}\n\n::: columns\n::: {.column width=\"70%\"}\n::: {.column width=\"70%\"}\n**Start Small**\n\n-   Share a new (small) dataset (that was not previously available).\n\n**Happy Medium**\n\n-   Share a new dataset with the AKRO through the ArcGIS online portal.\n\n**Bring It On!**\n\n-   Develop custom portals for viewing and interacting with your data.\n:::\n:::\n\n::: {.column width=\"30%\"}\n![](DataManagementAccessible_files/images/to_do_list.jpg)\n:::\n:::\n\n## Integrate/Analyze {.smaller}\n\n::: columns\n::: {.column width=\"30%\"}\n![](https://imgs.xkcd.com/comics/data_point.png)\n:::\n\n::: {.column width=\"70%\"}\nFor an analyses, you might be working with a single data set or integrating a number of datasets (e.g. sightings and environmental covariates).\n\n**Things to consider**:\n\n-   Identify and document those data within the documentation of the new derived data set.\n\n-   Make the extraction and integration of the data reproducible (e.g. stored query, stored output, programmatic extraction).\n\n-   Ensure any \"quirks\" in the data that are clearly understood by those analyzing the data, if that's not you.\n\n-   Think early about how you're going to share data and/or code for the analysis.\n:::\n:::\n\n## Integrate: Example #1 (getting data) {.smaller}\n\n**R package** (to simplify and ensure consistent retrieval from the database)\n\n![](DataManagementAccessible_files/images/pepDataConnect.png){fig-align=\"center\"}\n\n## Integrate/Analyze: Take-away Actions {.smaller}\n\n::: columns\n::: {.column width=\"70%\"}\n**Start Small**\n\n-   Make a folder for storing all the data products (and maybe code) for an analysis.\n\n**Happy Medium**\n\n-   Brainstorm a new way to work through an analyses with these considerations with a colleague.\n\n**Bring It On!**\n\n-   Use Git for storing all the data products (and maybe code) for an analysis.\n\n-   Develop an R package detailing the analyses (for code someone else might use).\n:::\n\n::: {.column width=\"30%\"}\n![](DataManagementAccessible_files/images/to_do_list.jpg)\n:::\n:::\n\n## Aerial Harbor Seal Surveys Example... {.smaller}\n\n1.  Regular standing data-related meetings throughout the year for planning/reviewing/ processing. After one field effort and before the next, evaluate any changes needed for data collection.\n\n2.  Survey areas are stored in the database for archive and tracking.\n\n3.  Data collection is standardized year-to-year, so import/data entry into the database is seamless. And the database is designed such that the fields are atomized and the structure is adaptable.\n\n4.  The counting process works directly with the DB -- allows for immediate QA/QC after counting is complete and for those data to be exported in one simple step for analysis.\n\n5.  The analytical results are ingested into the DB, and the only change required for the (future) Shiny app will be a change to the referenced data set.\n\n6.  InPort metadata records, datasets on NOAA Big Data Platform and ArcGIS online are updated.\n\n7.  We start the process again...\n\n## Data-related Positions {.smaller}\n\n![](DataManagementAccessible_files/images/hierarchy_of_needs.png){fig-align=\"center\"}\n\n-   Role is particularly important in the **planning** and **collecting** phases of the data management life cycle.\n\n-   The **assuring**, **describing**, **preserving** and **discovering** phases are the logical places where this role fits into the larger project.\n\n-   By the time the work gets to the **integrating** and **analyzing** phases, the person (or people) in this role knows the nuances of the data backwards and forwards.\n\n\\*Depending on the project/program structure, this might be one person or many people.\n\n## DEIA-related to Data Management {.smaller}\n\nMake data management practices more diverse, equitable, inclusive, and accessible:\n\n-   Target users of all technical backgrounds and develop systems that are accommodating to everyone.\n\n    -   Recognize different comfort levels of technology and different ways of doing things.\n    -   Don't want to leave anyone behind!\n    -   Create supportive environment for those struggles/challenges.\n    -   Make space for different ideas, perspectives, work styles, learning styles.\n\n-   Avoid gate-keeping of ideas, processes, workflows, data.\n\n    -   Increase transparency.\n    -   Think about data management as a complementary process to the research.\n    -   Work with one another -- all on the same team moving research efforts forward.\n    -   Make your data openly available.\n\n-   Recognize this work can be un(der)funded and/or un(der)staffed...\n\n## Reminders {.smaller}\n\nFor data management (and life):\n\n-   Keep it simple.\n\n-   Start where you are. And *don't* feel embarrassed or shamed by wherever that is.\n\n-   Not everything has to be a \"stretch exercise.\"\n\n-   Not every project/problem/situation will have the same technology solution. Find and use the best technology for the task at hand.\n\n-   Don't compare yourself to other people.\n\n-   Don't be afraid to try new things or fail the first (or first few times) you try something new.\n\n    -   My data-related nemesis is date/time time zones. I *still* get them wrong more than I'd like to admit and have to check myself. Every. Single. Time.\n\n-   Celebrate your successes!\n\n-   Ask others for help, if you're stuck or want a different perspective.\n\n## Next Steps {.smaller}\n\n-   This information can be applied to your individual work or to projects.\n\n    -   For individual files and information, learn from one another!\n    -   For projects, this should be a team effort.\n        -   No one person can do all the things.\n\n        -   Be clear and intentional.\n\n        -   None of the examples presented in these slides were done by me in isolation!\n\n-   This presentation may have been a little or a lot of new information for you.\n\n    -   If it was not a lot of new information for you, you're off to a great start! Share your knowledge with your colleagues and help them get more comfortable.\n    -   If it was a lot of new information for you (and maybe you're feeling overwhelmed with where to start), pick one thing from one section and give it a try.\n\n-   I am *always* happy to talk about data things. Feel free to ask questions, run things by me, etc.\n\n## Resources {.smaller}\n\n-   [PEP Data Management Plan](https://github.com/staciekoslovsky-noaa/PEP_Admin/blob/main/PEP_DataMgmtPlan.qmd)\n-   [Biological Observation Data Standardization - A Primer for Data Managers](https://esip.figshare.com/articles/online_resource/Biological_Observation_Data_Standardization_-_A_Primer_for_Data_Managers/16806712/2)\n-   [DataOne Primer on Data Management](https://repository.oceanbestpractices.org/bitstream/handle/11329/502/DataONE_BP_Primer_020212.pdf)\n-   [Harvard Biomedical Data Lifecycle](https://datamanagement.hms.harvard.edu/plan-design/biomedical-data-lifecycle)\n-   [NOAA Data Strategy: Maximizing the Value of NOAA Data](https://sciencecouncil.noaa.gov/Portals/0/2020%20Data%20Strategy.pdf?ver=2020-09-17-150024-997)\n-   [ISO 8601 Standards for date/time](https://en.wikipedia.org/wiki/ISO_8601#Dates)\n\nThank you to everyone who contributed ideas for helping to formulate parts of this talk!\n",
    "supporting": [
      "DataManagementAccessible_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/htmlwidgets-1.5.4/htmlwidgets.js\"></script>\r\n<script src=\"site_libs/viz-1.8.2/viz.js\"></script>\r\n<link href=\"site_libs/DiagrammeR-styles-0.2/styles.css\" rel=\"stylesheet\" />\r\n<script src=\"site_libs/grViz-binding-1.0.9/grViz.js\"></script>\r\n"
      ],
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}